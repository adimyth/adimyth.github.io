<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Custom JS -->
    <script src="assets/js/highlight.pack.js"></script>

    <!-- Google Font-->
    <link href="//fonts.googleapis.com/css?family=Roboto" rel="stylesheet">


    <title>MultiProcessing in Python</title>
</head>

<body class="container">
    <h1 id="decisiontrees">Decision Trees</h1>
    <div class="table-responsive">
        <table class="table">
            <thead class="thead-dark">
                <tr>
                    <th>A</th>
                    <th>B</th>
                    <th>C</th>
                    <th>D</th>
                    <th>E</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>4.8</td>
                    <td>3.4</td>
                    <td>1.9</td>
                    <td>0.2</td>
                    <td>positive</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td>3</td>
                    <td>1.6</td>
                    <td>0.2</td>
                    <td>positive</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td>3.4</td>
                    <td>1.6</td>
                    <td>0.4</td>
                    <td>positive</td>
                </tr>
                <tr>
                    <td>5.2</td>
                    <td>3.5</td>
                    <td>1.5</td>
                    <td>0.2</td>
                    <td>positive</td>
                </tr>
                <tr>
                    <td>5.2</td>
                    <td>3.4</td>
                    <td>1.4</td>
                    <td>0.2</td>
                    <td>positive</td>
                </tr>
                <tr>
                    <td>4.7</td>
                    <td>3.2</td>
                    <td>1.6</td>
                    <td>0.2</td>
                    <td>positive</td>
                </tr>
                <tr>
                    <td>4.8</td>
                    <td>3.1</td>
                    <td>1.6</td>
                    <td>0.2</td>
                    <td>positive</td>
                </tr>
                <tr>
                    <td>5.4</td>
                    <td>3.4</td>
                    <td>1.5</td>
                    <td>0.4</td>
                    <td>positive</td>
                </tr>
                <tr>
                    <td>7</td>
                    <td>3.2</td>
                    <td>4.7</td>
                    <td>1.4</td>
                    <td>negative</td>
                </tr>
                <tr>
                    <td>6.4</td>
                    <td>3.2</td>
                    <td>4.5</td>
                    <td>1.5</td>
                    <td>negative</td>
                </tr>
                <tr>
                    <td>6.9</td>
                    <td>3.1</td>
                    <td>4.9</td>
                    <td>1.5</td>
                    <td>negative</td>
                </tr>
                <tr>
                    <td>5.5</td>
                    <td>2.3</td>
                    <td>4</td>
                    <td>1.3</td>
                    <td>negative</td>
                </tr>
                <tr>
                    <td>6.5</td>
                    <td>2.8</td>
                    <td>4.6</td>
                    <td>1.5</td>
                    <td>negative</td>
                </tr>
                <tr>
                    <td>5.7</td>
                    <td>2.8</td>
                    <td>4.5</td>
                    <td>1.3</td>
                    <td>negative</td>
                </tr>
                <tr>
                    <td>6.3</td>
                    <td>3.3</td>
                    <td>4.7</td>
                    <td>1.6</td>
                    <td>negative</td>
                </tr>
                <tr>
                    <td>4.9</td>
                    <td>2.4</td>
                    <td>3.3</td>
                    <td>1</td>
                    <td>negative</td>
                </tr>
            </tbody>
        </table>
    </div>

    <h2 id="informationgain">Information Gain</h2>

    <p>Attribute with high information gain is placed at the root.</p>

    <ul>
        <li>Information gain (IG) measures how much “information” a feature gives us about the class.
            <img class="img-fluid" src="https://cdn-images-1.medium.com/max/800/1*bVGWGETTor7bSnhr7sXEVw.png"
                alt="Information Gain" />
        </li>

        <li>Entropy is the measures of impurity, disorder or uncertainty in a bunch of examples.


            <ul>
                <li>If all examples are positive or negative, then the entropy is 0</li>

                <li>If half the records are positive &amp; half are negative, then entropy is 1</li>
            </ul>
        </li>
    </ul>

    <p><img class="img-fluid"
            src="https://wikimedia.org/api/rest_v1/media/math/render/svg/971ffd75b32f284123036d4ae8fc3dd6e377e030"
            alt="Entropy" title="Entropy" /></p>

    <h3 id="step1">Step 1</h3>

    <p>Categorizing the continous data</p>
    <table class="table">
        <thead class="thead-dark">
            <tr>
                <th>A</th>
                <th>B</th>
                <th>C</th>
                <th>D</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>&gt;=5</td>
                <td>&gt;=3.0</td>
                <td>&gt;=4.2</td>
                <td>&gt;=1.4</td>
            </tr>
            <tr>
                <td>&lt;5</td>
                <td>&lt;3</td>
                <td>&lt;4.2</td>
                <td>&lt;1.4</td>
            </tr>
        </tbody>
    </table>

    <h3 id="step2">Step 2</h3>

    <p>Calculating the entropy of the target</p>

    <pre><code>E(8,8) = -1*( (p(+ve)*log( p(+ve)) + (p(-ve)*log( p(-ve)) )
                = -1*( (8/16)*log2(8/16)) + (8/16) * log2(8/16) )
                = 1
        </code></pre>

    <h3 id="step3">Step 3</h3>

    <p>Calculate information gain for each of the attributes</p>

    <h4 id="vara">Var A</h4>

    <p>A >= 5</p>

    <pre><code>
        A &gt;=5 &amp; E == positive : 5/12
        A &gt;=5 &amp; E == negative: 7/12
        Entropy(A&gt;=5) = -[5/12*log(5/12) + 7/12*log(7/12)] = 0.9799
    </code></pre>

    <p>E(A>=5) = 0.9799</p>

    <p>A &lt; 5</p>

    <pre><code>
        A &lt; 5 &amp; E == positive : 3/4
        A &lt; 5 &amp; E == negative: 1/4
        Entropy(A&gt;=5) = -[3/4*log(3/4) + 1/4*log(1/4)] = 0.8113
    </code></pre>

    <p>E(A&lt;=5) = 0.8113</p>

    <blockquote class="blockquote">
        <p>Entropy(Target, A) = (12/16)0.9799 + (4/16)0.8113 = 0.937745</br>
            Information Gain(IG) = E(target) - E(Target, A) = 1 - 0.937745 = 0.062255</p>

        <h4 id="varb">Var B</h4>

        <p>Information Gain(IG) = E(target) - E(Target, B) = 1 - 0.292905 = 0.70709</p>

        <h4 id="varc">Var C</h4>

        <p>Information Gain(IG) = E(target) - E(Target, C) = 1 - 0.4512 = 0.5488</p>

        <h4 id="vard">Var D</h4>

        <p>Information Gain(IG) = E(target) - E(Target, D) = 1 - 0.84532 = 0.5811</p>
    </blockquote>

    <h2 id="giniindex">Gini Index</h2>

    <p>Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified. It means
        an attribute with lower gini index should be preferred.
        <img src="http://www.learnbymarketing.com/wp-content/uploads/2016/02/gini-index-formula.png" alt="Gini Index" />
    </p>

    <h3 id="step1-1">Step 1</h3>

    <p>Same as above</p>

    <h3 id="step2-1">Step 2</h3>

    <p>Calculate Gini Index</p>

    <h4 id="vara-1">Var A</h4>

    <p>A >= 5</p>

    <pre><code>
        A &gt;=5 &amp; E == positive : 5/12 
        A &gt;=5 &amp; E == negative : 7/12 
        Gini(A, +ve) = 1 - [square(5/12) + square(7/12)] = 0.4860
    </code></pre>

    <p>A &lt; 5</p>

    <pre><code>
        A &lt; 5 &amp; E == positive : 3/4 
        A &lt; 5 &amp; E == negative : 1/4 
        Gini(A, +ve) = 1 - [square(3/4) + square(1/4)] = 0.375
    </code></pre>

    <blockquote class="blockquote">
        <p>Gini Index(Target, A) = (12/16)* 0.4860 + (4/16)*0.375 = 0.458</p>

        <h4 id="varb-1">Var B</h4>

        <p>Gini Index(Target, B) = (12/16)* 0.446 + (4/16)*0 = 0.3345</p>

        <h4 id="varc-1">Var C</h4>

        <p>Gini Index(Target, C) = (6/16)* 0 + (10/16)*0.32 = 0.2</p>

        <h4 id="vard-1">Var D</h4>

        <p>Gini Index(Target, D) = (5/16)* 0 + (11/16)*0.38 = 0.273</p>
    </blockquote>

    <h1 id="randomforest">Random Forest</h1>

    <p>In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap
        sample) from the training set. In addition, when splitting a node during the construction of the tree, the split
        that is chosen is no longer the best split among all features. Instead, the split that is picked is the best
        split among a random subset of the features. As a result of this randomness, the bias of the forest usually
        slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance
        also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.
    </p>

    <p>In contrast to the original publication, the scikit-learn implementation combines classifiers by averaging their
        probabilistic prediction, instead of letting each classifier vote for a single class.</p>

    <h3 id="randomness">Randomness</h3>

    <p>Randomness is at two levels:</p>

    <ol>
        <li>Row Level => Each decision tree is trained only on a subset of data.</li>

        <li>Column Level => The split is made up from only a subset of features, instead of all the features.</li>
    </ol>

    <h3 id="disadvantages">Disadvantages</h3>

    <ol>
        <li>
            <p>Random forests don't train well on smaller datasets as it fails to pick on the pattern.</p>
        </li>

        <li>
            <p>The time taken to train random forests may sometimes be too huge as you train multiple decision trees.
                Also, in the case of a categorical variable, the time complexity increases exponentially. For a
                categorical column with n levels, RF tries split at 2^n -1 points to find the maximal splitting point.
            </p>
        </li>

        <li>
            <p>For a regression problem, the range of values the output variable can take is determined by the values
                already available in the training dataset. Unlike linear regression, decision trees and hence random
                forest can't take values outside the training data.</p>
        </li>
    </ol>

    <h3 id="advantages">Advantages</h3>

    <ol>
        <li>
            <p>Since we are using multiple decision trees, the bias remains same as that of a single decision tree.
                However, the variance decreases and thus we decrease the chances of overfitting.</p>
        </li>

        <li>
            <p>Easy and quick to use for getting ground up results.</p>
        </li>
    </ol>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>
</body>

</html>