<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Custom JS -->
    <script src="assets/js/highlight.pack.js"></script>

    <!-- Google Font-->
    <link href="//fonts.googleapis.com/css?family=Roboto" rel="stylesheet">


    <title>MultiProcessing in Python</title>
</head>

<body class="container">
    <h2 id="word2vec">Word2Vec</h2>

    <p>NLP systems treat words as discrete atomic symbols. These encodings are arbitrary, and provide no information to
        the system regarding the relationships that may exist between the individual symbols.
        Representing words as unique, discrete ids leads to data sparsity, and we may need more data in order to
        sucessfully train statistical models.</p>

    <p>Vector space models (VSM) represent words in a continuous vector space where semantically similar words are
        mapped to nearby points.
        Distributional Hypothesis, states that the words which appear in same context share semantic meaning.</p>

    <p>Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large
        text corpus, and then map these count-statistics down to a small, dense vector for each word.</p>

    <p>Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding
        vectors.</p>

    <p>Continuous Bag of Words (CBOW) works very well for smaller dataset. However, skip-gram tends to perform better
        when we have larger datasets.</p>

    <p>For a corpus of vocabulary size |V| and n components (dimension vector size), the total number of weights is |V|
        * n in the hidden layer and output layer each. Running gradient descent on a neural network that large is going
        to be slow. And fine tuning that many weights requires huge amount of training data.</p>

    <h3 id="solution">Solution</h3>

    <h4 id="phrasedetection">Phrase Detection</h4>

    <p>The authors pointed out that a word pair like “Boston Globe” (a newspaper) has a much different meaning than the
        individual words “Boston” and “Globe”. So it makes sense to treat “Boston Globe”, wherever it occurs in the
        text, as a single word with its own word vector representation.</p>

    <ol>
        <li>
            <p>In addition to all the unique words in the training set, we also add every combination of two words
                observed in the text. For example, if we have a sentence "I love pizza", then we add vocabulary entries
                and counts for "I", "love", "pizza", "I<em>love", and "love</em>pizza".</p>
        </li>

        <li>
            <p>We are trying to determine whether words A and B should be turned into A<em>B.
                    Pa -> word count of word A
                    Pb -> word count of word B
                    Pab -> word count of word 'AB'
                    min</em>count -> parameter to eliminate infrequent phrases
                <code>
        ratio = (Pab - min_count) / (Pa * Pb)
        </code>
                Larger the ratio, more likely it is that the words A &amp; B occur together.</p>
        </li>
    </ol>

    <h4 id="subsamplingfrequentwords">Subsampling Frequent Words</h4>

    <p>There's a possibility that a word in the training text might be removed. It depends upon the word's frequency.
    </p>

    <p><code>P(w) = 1 - sqrt(t / fraction(w))</code>
        fraction(w) -> number of times w appears divided by the total number of words
        t -> threshold typically around 10<sup>-5</sup></p>

    <h4 id="negativesampling">Negative Sampling</h4>

    <p>Each training sample will tweak all the weights in the neural network. For billions of weights, it is very
        costly. Negative sampling addresses this by having each training sample only modify a small percentage of
        weights.</p>

    <p>Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating
        the weights for our positive word, plus the weights for 5 other words that we want to output 0. That’s a total
        of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!</p>

    <p>Whereas, for the input hidden layer, only the weights for the input word are updated.</p>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>
</body>

</html>